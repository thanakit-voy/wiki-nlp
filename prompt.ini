
.\scripts\fetch.ps1
.\scripts\segment.ps1
.\scripts\thai_clock.ps1
.\scripts\sentences.ps1
.\scripts\sentence_token.ps1
.\scripts\tag_num.ps1
.\scripts\connectors.ps1
.\scripts\abbreviation.ps1
.\scripts\tokenize.ps1

เขียนสคริปต์ sentence_heads
เอาข้อมูล sentences จาก corpus collection
มาสร้างประโยค โดยอิงจาก ค่า head ดูจาก tokens

จากข้อมูลตัวอย่าง
  "sentences": [
    {
      "text": "รถโดยสารประจำทางมีหลายสายเพื่อเป็นการบริการประชาชน ให้บริการในราคาย่อมเยา",
      "tokens": [
        {
          "id": 1,
          "lemma": "รถโดยสารประจำทาง",
          "head": 2,
        },
        {
          "id": 2,
          "lemma": "มี",
          "lemma": "มี",
          "head": 0,
        },
        {
          "id": 3,
          "lemma": "หลาย",
          "head": 4,
        },
        {
          "id": 4,
          "pos": "NOUN",
          "lemma": "สาย",
          "head": 2,
        },
        {
          "id": 5,
          "lemma": "เพื่อ",
          "head": 6,
        },
        {
          "id": 6,
          "lemma": "เป็นการ",
          "head": 2,
        },
        {
          "id": 7,
          "lemma": "บริการ",
          "head": 6,
        },
        {
          "id": 8,
          "lemma": "ประชาชน",
          "head": 7,
        },
        {
          "id": 9,
          "lemma": "ให้บริการ",
          "depparse": "acl",
          "head": 8,
        },
        {
          "id": 10,
          "lemma": "ใน",
          "head": 11,
        },
        {
          "id": 11,
          "lemma": "ราคา",
          "head": 9,
        },
        {
          "id": 12,
          "lemma": "ย่อมเยา",
          "head": 11,
        }
      ]
    },
    
เราจะต้องหา unique head ซึ่งจะได้ คือ 0,2,4,6,7,9,11
จากนั้นนำเอาค่า head id แต่ละอันมาสร้างเป็นคำ (head = 0 ไม่นับ เพราะเป็น root)
เช่น 2 ดึงคำที่มี head = 2 และ id = 2 มา แต่ยังคงลำดับเดิมไว้
จะได้คำว่า "รถโดยสารประจำทาง มี สาย เป็นการ"  (เอา "lemma" มาต่อกันนะ)
id 4 จะได้ "หลาย สาย"
id 6 จะได้ "เพื่อ เป็นการ บริการ"
id 7 จะได้ "บริการ ประชาชน"
จากนั้นบันทึกเป็น sentence_heads
{
"sentences": [....],
"sentence_heads": [
    {
      "head": "มี",
      "text": "รถโดยสารประจำทาง มี สาย เป็นการ",
      tokens: [....]
    },
    {
      "head": "สาย",
      "text": "หลาย สาย",
      tokens: [....]
    },
  ....
  ]
}


ก่อนทำ ให้เช็คว่า process.sentence_heads มีค่าเป็น true หรือไม่ ถ้าใช่จะไม่ทำข้อมูลชุดนั้น
และหลังทำเสร็จแล้ว ให้ตั้งค่า process.sentence_heads เป็น true

ตัวอย่างข้อมูลแต่ละ collection อยู่ใน example_data.json

เขียนสคริปต์ connectors
เอาข้อมูล sentences จาก corpus collection 
มาพิจารณาว่่าจะนำประโยคมาต่อกันหรือไม่ โดยพิจารณาเงื่อนไขดังนี้
  ประโยคนี้สั้นเกินไปหรือไม่ (เช่น น้อยกว่า 25 ตัวอักษร)
  ประโยคก่อนหน้าสั้นเกินไปหรือไม่ (เช่น น้อยกว่า 25 ตัวอักษร)
  ประโยคนี้เป็นตัวเลขหรือไม่ เช็คจาก pos = "NUM"
  ประโยคนี้มีคำเชื่อมอยู่หน้าประโยคหรือไม่ (THAI_CONNECTORS_PREFIX)
  ประโยคก่อนหน้ามีคำเชื่อมอยู่ท้ายประโยคหรือไม่ (THAI_CONNECTORS_SUFFIX)
  ประโยคนี้มี OPENING_PUNCTS หรือไม่
  ประโยคนี้ขึ้นต้นด้วย THAI_NUMBER_WORDS หรือไม่
  หากมีเงื่อนไขสักข้อใดข้อหนึ่ง ให้รวมประโยคนี้กับประโยคก่อนหน้า (ดูค่าคงทๆานในไฟล์ constants.py)

ตัวอย่างข้อมูลแต่ละ collection อยู่ใน example_data.json

แต่รอบนี้ใช้ pythainlp.tokenize.sent_tokenize


เขียนสคริปต์ tokenize
เอาข้อมูล sentences จาก corpus collection 
มาแบ่งประโยคเป็นคำ ๆ พร้อมทั้งหา pos, lemma, depparse ด้วย Stanza
(รองรับ custom dict (data/input/custom_dict.txt) ด้วยนะ)

"sentences": [
      {
        "text": "ทางเชื่อมทางพิเศษศรีรัช กับทางพิเศษสายศรีรัช-วงแหวนรอบนอกกรุงเทพมหานคร ระยะทาง 360 เมตร เปิดให้บริการเมือวันที่ 30 กันยายน พ.ศ. 2561"
        "tokens": [
          {"text":"ทางเชื่อม", type:"NOUN", pos:"NOUN", lemma:"ทางเชื่อม", depparse:"nsubj", lang:"th"},
          {"text": "ทาง", type:"NOUN", pos:"NOUN", lemma:"ทาง", depparse:"nsubj", lang:"th"},
          {"text": "พิเศษ", type:"ADJ", pos:"ADJ", lemma:"พิเศษ", depparse:"amod", lang:"th"},
          {"text": "ศรีรัช", type:"NOUN", pos:"NOUN", lemma:"ศรีรัช", depparse:"nmod", lang:"th"},
          {"text": "กับ", type:"ADP", pos:"ADP", lemma:"กับ", depparse:"case", lang:"th"},
          {"text": "ทาง", type:"NOUN", pos:"NOUN", lemma:"ทาง", depparse:"nmod", lang:"th"},
          {"text": "พิเศษ", type:"ADJ", pos:"ADJ", lemma:"พิเศษ", depparse:"amod", lang:"th"},
          {"text": "สาย", type:"NOUN", pos:"NOUN", lemma:"สาย", depparse:"nmod", lang:"th"},
          ....
        ]
      }

      ERA_TOKENS = {"พุทธศักราช","คริสต์ศักราช", "จุลศักราช"}

ตรง type คือ เป็นการเช็คเพิ่มเติม ถ้า pos เป็น ตัวเลข ให้เช็ค รูปแบบคำว่าเป็น วันที่, เวลา, เงิน, ระยะทาง, ปริมมาตร, น้ำหนัก ..... หรือไม่ ก็ให้ กำหนด type ตามนั้น 
(อาจจะต้องมีการเช็คจาก คำก่อนหน้า และ หลังหน้า ด้วย อย่างเช่น เวลา มักจากตามด้วย "นาฬิกา" เป็นต้น)
และพวกคำอื่น ๆ อาจจะเช็คว่า เป็นชื่อเดือนไหม หรือว่าเป็นหน่วยไหม (เช่น กิโลเมตร, เมตร, กรัม, กิโลกรัม, ลิตร, มิลลิลิตร, บาท, ดอลลาร์, ยูโร, ปอนด์, เยน, หยวน ....)
ทำให้ครอบคลุมมากที่สุด ลองคิดวิธีการที่จะทำให้ได้ type ที่ถูกต้อง 
หากมีการกำหนด type ประเภทอื่น ๆ ที่น่าสนใจ ก็สามารถเพิ่มได้

ก่อนทำ ให้เช็คว่า process.tokenize มีค่าเป็น true หรือไม่ ถ้าใช่จะไม่ทำข้อมูลชุดนั้น
และหลังทำเสร็จแล้ว ให้ตั้งค่า process.tokenize เป็น true

ตัวอย่างข้อมูลแต่ละ collection อยู่ใน example_data.json



เขียนสคริปต์ abbreviation เพื่อแปลงคำย่อในประโยคเป็นคำเต็ม
นำประโยค แปลงตัวย่อเป็นคำ ใช้ pythainlp.util.abbreviation_to_full_text
ตัวอย่าง
text = "รร.ของเราน่าอยู่ใน พ.ศ. 2565"
abbreviation_to_full_text(text)
output: [
('โรงเรียนของเราน่าอยู่ใน พุทธศักราช 2565', tensor(0.3734)),
('โรงแรมของเราน่าอยู่ใน พุทธศักราช 2565', tensor(0.2438))
]
เลือกเอาคำที่ความน่าจะเป็นสูงสุด
จากการแปลงตัวย่อ คุณจะทราบถึงการเปลี่ยนแปลงของประโยค ถ้าประโยคไม่เหมือนเดิม แสดงว่ามีคำย่อ
ดังตัวอย่างข้างต้นจะพบว่า มีคำย่อ "รร." สามารถแปลงเป็น "โรงเรียน" หรือ "โรงแรม"
และ "พ.ศ." สามารถแปลงเป็น "พุทธศักราช"
ให้เก็บค่าไว้ เพื่อบันทึกคำย่อ (บันทึกทุกรูปแบบ) ลง db abbreviation collection

ก่อนทำ ให้เช็คว่า process.abbreviation มีค่าเป็น true หรือไม่ ถ้าใช่จะไม่ทำข้อมูลชุดนั้น
และหลังทำเสร็จแล้ว ให้ตั้งค่า process.abbreviation เป็น true

ตัวอย่างข้อมูลแต่ละ collection อยู่ใน example_data.json



เขียนสคริปต์ thai_clock เพื่อแปลงเวลาที่เป็นภาษาไทย
เอาข้อมูล raw.content จาก corpus collection ทำการเขียนทับข้อมูลเดิม
ก่อนทำ ให้เช็คว่า process.thai_clock มีค่าเป็น false หรือไม่ จึงจะทำข้อมูลชุดนั้น
และหลังทำเสร็จแล้ว ให้ตั้งค่า process.thai_clock เป็น true
โดยการตรวจจับ pattern ที่เป็นเวลา
    Rules:
      - Convert HH:MM to HH.MM
      - If followed by "น." or "น" (with a required space after in the latter case),
                replace that suffix with the word "นาฬิกา" (for colon-form times).
            - For dot-form times (HH.MM) followed by "น." or "น" (with optional spaces),
                replace the suffix with "นาฬิกา".

    Examples:
    "01:00 น." -> "01.00 นาฬิกา"
    "1:00 น "  -> "1.00 นาฬิกา"
    "01.30 น." -> "01.30 นาฬิกา"
    "01.30น "  -> "01.30 นาฬิกา"
    "1:20"     -> "1.20"
    """




ตัวอย่างเช่น

"sentences": [
    {
        "text": "กรุงเทพมหานครเป็นเมืองหลวงและเมืองที่มีประชากรมากที่สุดของประเทศไทย",
        "created_at": "2025-09-27T19:47:47Z"
      },
      {
        "text": "กรุงเทพมหานครเป็นจุดรวมของความเจริญทั้งในระดับประเทศและระดับภูมิภาคเอเชียตะวันออกเฉียงใต้",
        "created_at": "2025-09-27T19:47:47Z"
      },
    ]

สมมติถูกแยกเพิ่ม จาก pythainlp เป็น

"sentences": [
    {
        "text": "กรุงเทพมหานครเป็นเมืองหลวง",
        "created_at": "2025-09-27T19:47:47Z"
      },
      {
        "text": "และเมืองที่มีประชากรมากที่สุดของประเทศไทย",  << ประโยคนี้ถูกตัดออกมาใหม่ และแทรกเข้ามาใน sentences
        "created_at": "2025-09-27T19:47:47Z"
      },
      {
        "text": "กรุงเทพมหานครเป็นจุดรวมของความเจริญ",
        "created_at": "2025-09-27T19:47:47Z"
      },
      {
        "text": "ทั้งในระดับประเทศและระดับภูมิภาคเอเชียตะวันออกเฉียงใต้", << ประโยคนี้ถูกตัดออกมาใหม่ และแทรกเข้ามาใน sentences
        "created_at": "2025-09-27T19:47:47Z"
      },
    ]

ตัวอย่างข้อมูลแต่ละ collection อยู่ใน example_data.json



เขียนสคริปต์ ดูแต่ละประโยค จาก corpus collection 
ประโยคนั้นอยู่ในรูปแบบของภาษาอื่นที่ไม่ใช่ภาษาไทยไหม
อักขระพิเศษที่เแ็นเครื่องหมายถือว่าไม่เอามาคิด เช่น "(DOG" "C_A_T" "Rab-bit" แม้ว่าในนั้นจะมี ( _ - ก็ตาม ก็ยังถือว่าเป็นคำต่างประเทศ
ถ้าเป็นคำต่างประเทศ ให้ทำการระบุ type เป็น "NONTH" และ pos เป็น "NONTH"
และถ้าเป็นอักขระพิเศษ ให้ระบุ type เป็น "SYM" และ pos เป็น "SYM" ไม่ว่าจะมีกี่กี่ตัวก็ตาม แต่ถ้าเป็นตัวอักษรพิเศษทั้งหมด เช่น "!!!" "??" "+" "-" "+"" """
"sentences": [
    {
        "text": "ANIMAL",
        type: "NONTH",
        pos: "NONTH",
        "created_at": "2025-09-27T19:47:47Z"
      },
      {
        "text": "(",
        type: "SYM",
        pos: "SYM",
        "created_at": "2025-09-27T19:47:47Z"
      },
      {
        "text": "ICON-SIAM",
        type: "NONTH",
        pos: "NONTH",
        "created_at": "2025-09-27T19:47:47Z"
      },
      {
        "text": ")",
        type: "SYM",
        pos: "SYM",
        "created_at": "2025-09-27T19:47:47Z"
      },
    ]
}

ตัวอย่างข้อมูลแต่ละ collection อยู่ใน example_data.json